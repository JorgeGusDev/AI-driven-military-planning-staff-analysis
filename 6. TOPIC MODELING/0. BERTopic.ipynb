{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf901e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ed225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar stopwords de NLTK si no están\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd881376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta donde están los archivos\n",
    "CARPETA = r\"C:\\Users\\LENOVO\\OneDrive - Grupo Educad\\7. IDEA_LAB\\7. SNA & SA\\4. PAPER_SNA_SA\\1. Data_Technics & Outputs\\2. NLP_SENTIMENTAL ANALYSIS\\NLP_v2\\Resultados_NLP_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los archivos Excel\n",
    "archivos = glob.glob(os.path.join(CARPETA, \"*.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be45a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords en español + inglés + personalizadas\n",
    "stopwords_es = stopwords.words(\"spanish\")\n",
    "stopwords_en = list(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ba774",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = [\n",
    "    \"hola\",\"buenos\",\"buenas\",\"tarde\",\"noche\",\"gracias\",\"favor\",\"ok\",\"vale\",\"aja\",\"jaja\",\"jeje\",\"ajá\",\"eh\",\"mmm\",\"xd\",\"jajaja\",\n",
    "    \"sr\",\"sra\",\"señor\",\"señora\",\"señores\",\"jefe\",\"compañeros\",\"compañeras\",\"grupo\",\"equipo\",\"gente\",\"personas\",\n",
    "    \"sí\",\"no\",\"ya\",\"aquí\",\"allí\",\"ahí\",\"entonces\",\"también\",\"además\",\"bueno\",\"pues\",\"este\",\"osea\",\"ose\",\"ehh\",\"aja\",\n",
    "    \"día\",\"días\",\"semana\",\"mes\",\"año\",\"tiempo\",\"momento\",\"cosas\",\"algo\",\"nada\",\"todo\",\"todos\",\"todas\",\"algún\",\"algunos\",\"algunas\",\n",
    "    \"cahue\",\"video\",\"omitido\",\"eliminó\",\"videos\",\"rdo\",\"mensaje\",\"11\",\"noches\",\"haz click\",\"unirte reunion\",\"google\",\"si\",\"jerry\",\n",
    "    \"jpg\",\"haz\",\"fotos\",\"mas\",\"dia\",\"galindo\",\"gandulias\",\"peñaranda\",\"crl\",\"12 12\",\"12\",\"cta\",\"llamada\",\"celular 13\",\"chavez\",\n",
    "    \"aler\",\"mayor\",\"mayores\",\"audios\",\"moreno\",\"audio\",\"olivera solari\",\"algonasi\", \"eliminaste\", \"solari\", \"capulian\", \"10\", \"chávez\",\n",
    "    \"tarjeta contacto\", \"contacto omitida\", \"abrazo\", \"rodrígues\",\"alex\",\n",
    "    \"unirte\", \"clic vínculo\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_custom = list(set(stopwords_es + stopwords_en + extra_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ccb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizador\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stopwords_custom,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa076a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada archivo\n",
    "for archivo in archivos:\n",
    "    print(f\"\\nProcesando archivo: {archivo}\")\n",
    "    df = pd.read_excel(archivo, sheet_name=0)\n",
    "    \n",
    "    # Solo la columna 'phrase'\n",
    "    docs = df[\"phrase\"].dropna().astype(str).tolist()\n",
    "    if len(docs) == 0:\n",
    "        print(\"Archivo vacío, se omite.\")\n",
    "        continue\n",
    "\n",
    "    # Crear modelo BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        language=\"multilingual\",\n",
    "        vectorizer_model=vectorizer,\n",
    "        min_topic_size=10,\n",
    "        calculate_probabilities=True\n",
    "    )\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    \n",
    "    # Reducir tópicos\n",
    "    topic_model = topic_model.reduce_topics(docs, nr_topics=2)\n",
    "    \n",
    "    # Info de tópicos\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topic_info = topic_info[topic_info.Topic != -1]  # excluir -1\n",
    "    print(topic_info.head(3))\n",
    "    \n",
    "    # Imprimir análisis de Beta (palabras y su peso por tópico)\n",
    "    print(\"\\nAnálisis de Beta por tópico:\")\n",
    "    for topic_num in topic_info.Topic.tolist():\n",
    "        print(f\"\\nTópico {topic_num}:\")\n",
    "        beta_words = topic_model.get_topic(topic_num)\n",
    "        for word, weight in beta_words:\n",
    "            if weight > 0:\n",
    "                print(f\"{word}: {weight:.4f}\")\n",
    "\n",
    "    \n",
    "    # Plot tipo LDA (barchart)\n",
    "    fig = topic_model.visualize_barchart(top_n_topics=1, n_words=10)\n",
    "    fig.update_layout(title_text=os.path.basename(archivo))  # título = nombre del archivo\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
